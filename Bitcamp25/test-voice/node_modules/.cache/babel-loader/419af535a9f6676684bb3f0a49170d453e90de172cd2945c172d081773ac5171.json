{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { _FusedMatMul, broadcast_util } from '@tensorflow/tfjs-core';\nimport { FusableActivation } from './types';\nlet wasmFusedMatMul;\nfunction setup(backend) {\n  wasmFusedMatMul = backend.wasm.cwrap(_FusedMatMul, null /* void */, ['number', 'array', 'number', 'number', 'array', 'number', 'number', 'number', 'number', 'number', 'number', 'number', 'number' // out_id\n  ]);\n}\nfunction fusedBatchMatMul(args) {\n  const {\n    inputs,\n    backend,\n    attrs\n  } = args;\n  const {\n    a,\n    b,\n    bias,\n    preluActivationWeights\n  } = inputs;\n  if (a.dtype !== 'float32' || b.dtype !== 'float32') {\n    throw new Error(`_FusedMatMul for non non-float32 tensors not yet supported.`);\n  }\n  const {\n    transposeA,\n    transposeB,\n    activation,\n    leakyreluAlpha\n  } = attrs;\n  const aId = backend.dataIdMap.get(a.dataId).id;\n  const bId = backend.dataIdMap.get(b.dataId).id;\n  let biasId = 0;\n  if (bias != null) {\n    const biasData = backend.dataIdMap.get(bias.dataId);\n    if (biasData.shape.length !== 1) {\n      throw new Error(`_FusedMatMul only supports rank-1 bias but got ` + `rank ${biasData.shape.length}.`);\n    }\n    biasId = biasData.id;\n  }\n  const preluActivationWeightsId = preluActivationWeights == null ? 0 : backend.dataIdMap.get(preluActivationWeights.dataId).id;\n  const fusedActivation = FusableActivation[activation];\n  if (fusedActivation == null) {\n    throw new Error(`${activation} activation not yet supported for FusedConv2D ` + `in the wasm backend.`);\n  }\n  const leftDim = transposeA ? a.shape[2] : a.shape[1];\n  const rightDim = transposeB ? b.shape[1] : b.shape[2];\n  const batchDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const out = backend.makeOutput([...batchDims, leftDim, rightDim], a.dtype);\n  const outId = backend.dataIdMap.get(out.dataId).id;\n  const aShapeBytes = new Uint8Array(new Int32Array(a.shape).buffer);\n  const bShapeBytes = new Uint8Array(new Int32Array(b.shape).buffer);\n  wasmFusedMatMul(aId, aShapeBytes, a.shape.length, bId, bShapeBytes, b.shape.length, transposeA, transposeB, fusedActivation, biasId, preluActivationWeightsId, leakyreluAlpha || 0, outId);\n  return out;\n}\nexport const _fusedMatMulConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'wasm',\n  setupFunc: setup,\n  kernelFunc: fusedBatchMatMul\n};","map":{"version":3,"names":["_FusedMatMul","broadcast_util","FusableActivation","wasmFusedMatMul","setup","backend","wasm","cwrap","fusedBatchMatMul","args","inputs","attrs","a","b","bias","preluActivationWeights","dtype","Error","transposeA","transposeB","activation","leakyreluAlpha","aId","dataIdMap","get","dataId","id","bId","biasId","biasData","shape","length","preluActivationWeightsId","fusedActivation","leftDim","rightDim","batchDims","assertAndGetBroadcastShape","slice","out","makeOutput","outId","aShapeBytes","Uint8Array","Int32Array","buffer","bShapeBytes","_fusedMatMulConfig","kernelName","backendName","setupFunc","kernelFunc"],"sources":["C:\\Users\\kheri\\Downloads\\Bitcamp\\bitcamp-2025\\Bitcamp25\\tfjs-backend-wasm\\src\\kernels\\_FusedMatMul.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, broadcast_util, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {BackendWasm} from '../backend_wasm';\n\nimport {FusableActivation} from './types';\n\nlet wasmFusedMatMul:\n    (aId: number, aShape: Uint8Array, aShapeSize: number, bId: number,\n     bShape: Uint8Array, bShapeSize: number, transposeA: boolean,\n     transposeB: boolean, activation: number, biasId: number,\n     preluActivationWeightsId: number, leakyreluAlpha: number, outId: number) =>\n        void;\n\nfunction setup(backend: BackendWasm) {\n  wasmFusedMatMul = backend.wasm.cwrap(_FusedMatMul, null /* void */, [\n    'number',  // a_id\n    'array',   // a_shape\n    'number',  // a_shape.length\n    'number',  // b_id\n    'array',   // b_shape\n    'number',  // b_shape.length\n    'number',  // transpose_a\n    'number',  // transpose_b\n    'number',  // activation\n    'number',  // biasId\n    'number',  // preluActivationWeightsId\n    'number',  // leakyreluAlpha\n    'number'   // out_id\n  ]);\n}\n\nfunction fusedBatchMatMul(args: {\n  inputs: _FusedMatMulInputs,\n  backend: BackendWasm,\n  attrs: _FusedMatMulAttrs\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b, bias, preluActivationWeights} = inputs;\n\n  if (a.dtype !== 'float32' || b.dtype !== 'float32') {\n    throw new Error(\n        `_FusedMatMul for non non-float32 tensors not yet supported.`);\n  }\n\n  const {transposeA, transposeB, activation, leakyreluAlpha} = attrs;\n  const aId = backend.dataIdMap.get(a.dataId).id;\n  const bId = backend.dataIdMap.get(b.dataId).id;\n\n  let biasId = 0;\n  if (bias != null) {\n    const biasData = backend.dataIdMap.get(bias.dataId);\n    if (biasData.shape.length !== 1) {\n      throw new Error(\n          `_FusedMatMul only supports rank-1 bias but got ` +\n          `rank ${biasData.shape.length}.`);\n    }\n    biasId = biasData.id;\n  }\n  const preluActivationWeightsId = preluActivationWeights == null ?\n      0 :\n      backend.dataIdMap.get(preluActivationWeights.dataId).id;\n  const fusedActivation =\n      FusableActivation[activation as unknown as\n                        keyof typeof FusableActivation];\n  if (fusedActivation == null) {\n    throw new Error(\n        `${activation} activation not yet supported for FusedConv2D ` +\n        `in the wasm backend.`);\n  }\n\n  const leftDim = transposeA ? a.shape[2] : a.shape[1];\n  const rightDim = transposeB ? b.shape[1] : b.shape[2];\n  const batchDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n\n  const out = backend.makeOutput([...batchDims, leftDim, rightDim], a.dtype);\n  const outId = backend.dataIdMap.get(out.dataId).id;\n\n  const aShapeBytes = new Uint8Array(new Int32Array(a.shape).buffer);\n  const bShapeBytes = new Uint8Array(new Int32Array(b.shape).buffer);\n\n  wasmFusedMatMul(\n      aId, aShapeBytes, a.shape.length, bId, bShapeBytes, b.shape.length,\n      transposeA, transposeB, fusedActivation, biasId, preluActivationWeightsId,\n      leakyreluAlpha || 0, outId);\n\n  return out;\n}\n\nexport const _fusedMatMulConfig: KernelConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'wasm',\n  setupFunc: setup,\n  kernelFunc: fusedBatchMatMul as unknown as KernelFunc\n};\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,YAAY,EAAyCC,cAAc,QAAiC,uBAAuB;AAInI,SAAQC,iBAAiB,QAAO,SAAS;AAEzC,IAAIC,eAKQ;AAEZ,SAASC,KAAKA,CAACC,OAAoB;EACjCF,eAAe,GAAGE,OAAO,CAACC,IAAI,CAACC,KAAK,CAACP,YAAY,EAAE,IAAI,CAAC,YAAY,CAClE,QAAQ,EACR,OAAO,EACP,QAAQ,EACR,QAAQ,EACR,OAAO,EACP,QAAQ,EACR,QAAQ,EACR,QAAQ,EACR,QAAQ,EACR,QAAQ,EACR,QAAQ,EACR,QAAQ,EACR,QAAQ,CAAG;EAAA,CACZ,CAAC;AACJ;AAEA,SAASQ,gBAAgBA,CAACC,IAIzB;EACC,MAAM;IAACC,MAAM;IAAEL,OAAO;IAAEM;EAAK,CAAC,GAAGF,IAAI;EACrC,MAAM;IAACG,CAAC;IAAEC,CAAC;IAAEC,IAAI;IAAEC;EAAsB,CAAC,GAAGL,MAAM;EAEnD,IAAIE,CAAC,CAACI,KAAK,KAAK,SAAS,IAAIH,CAAC,CAACG,KAAK,KAAK,SAAS,EAAE;IAClD,MAAM,IAAIC,KAAK,CACX,6DAA6D,CAAC;;EAGpE,MAAM;IAACC,UAAU;IAAEC,UAAU;IAAEC,UAAU;IAAEC;EAAc,CAAC,GAAGV,KAAK;EAClE,MAAMW,GAAG,GAAGjB,OAAO,CAACkB,SAAS,CAACC,GAAG,CAACZ,CAAC,CAACa,MAAM,CAAC,CAACC,EAAE;EAC9C,MAAMC,GAAG,GAAGtB,OAAO,CAACkB,SAAS,CAACC,GAAG,CAACX,CAAC,CAACY,MAAM,CAAC,CAACC,EAAE;EAE9C,IAAIE,MAAM,GAAG,CAAC;EACd,IAAId,IAAI,IAAI,IAAI,EAAE;IAChB,MAAMe,QAAQ,GAAGxB,OAAO,CAACkB,SAAS,CAACC,GAAG,CAACV,IAAI,CAACW,MAAM,CAAC;IACnD,IAAII,QAAQ,CAACC,KAAK,CAACC,MAAM,KAAK,CAAC,EAAE;MAC/B,MAAM,IAAId,KAAK,CACX,iDAAiD,GACjD,QAAQY,QAAQ,CAACC,KAAK,CAACC,MAAM,GAAG,CAAC;;IAEvCH,MAAM,GAAGC,QAAQ,CAACH,EAAE;;EAEtB,MAAMM,wBAAwB,GAAGjB,sBAAsB,IAAI,IAAI,GAC3D,CAAC,GACDV,OAAO,CAACkB,SAAS,CAACC,GAAG,CAACT,sBAAsB,CAACU,MAAM,CAAC,CAACC,EAAE;EAC3D,MAAMO,eAAe,GACjB/B,iBAAiB,CAACkB,UAC8B,CAAC;EACrD,IAAIa,eAAe,IAAI,IAAI,EAAE;IAC3B,MAAM,IAAIhB,KAAK,CACX,GAAGG,UAAU,gDAAgD,GAC7D,sBAAsB,CAAC;;EAG7B,MAAMc,OAAO,GAAGhB,UAAU,GAAGN,CAAC,CAACkB,KAAK,CAAC,CAAC,CAAC,GAAGlB,CAAC,CAACkB,KAAK,CAAC,CAAC,CAAC;EACpD,MAAMK,QAAQ,GAAGhB,UAAU,GAAGN,CAAC,CAACiB,KAAK,CAAC,CAAC,CAAC,GAAGjB,CAAC,CAACiB,KAAK,CAAC,CAAC,CAAC;EACrD,MAAMM,SAAS,GAAGnC,cAAc,CAACoC,0BAA0B,CACvDzB,CAAC,CAACkB,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAEzB,CAAC,CAACiB,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EAE/C,MAAMC,GAAG,GAAGlC,OAAO,CAACmC,UAAU,CAAC,CAAC,GAAGJ,SAAS,EAAEF,OAAO,EAAEC,QAAQ,CAAC,EAAEvB,CAAC,CAACI,KAAK,CAAC;EAC1E,MAAMyB,KAAK,GAAGpC,OAAO,CAACkB,SAAS,CAACC,GAAG,CAACe,GAAG,CAACd,MAAM,CAAC,CAACC,EAAE;EAElD,MAAMgB,WAAW,GAAG,IAAIC,UAAU,CAAC,IAAIC,UAAU,CAAChC,CAAC,CAACkB,KAAK,CAAC,CAACe,MAAM,CAAC;EAClE,MAAMC,WAAW,GAAG,IAAIH,UAAU,CAAC,IAAIC,UAAU,CAAC/B,CAAC,CAACiB,KAAK,CAAC,CAACe,MAAM,CAAC;EAElE1C,eAAe,CACXmB,GAAG,EAAEoB,WAAW,EAAE9B,CAAC,CAACkB,KAAK,CAACC,MAAM,EAAEJ,GAAG,EAAEmB,WAAW,EAAEjC,CAAC,CAACiB,KAAK,CAACC,MAAM,EAClEb,UAAU,EAAEC,UAAU,EAAEc,eAAe,EAAEL,MAAM,EAAEI,wBAAwB,EACzEX,cAAc,IAAI,CAAC,EAAEoB,KAAK,CAAC;EAE/B,OAAOF,GAAG;AACZ;AAEA,OAAO,MAAMQ,kBAAkB,GAAiB;EAC9CC,UAAU,EAAEhD,YAAY;EACxBiD,WAAW,EAAE,MAAM;EACnBC,SAAS,EAAE9C,KAAK;EAChB+C,UAAU,EAAE3C;CACb","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}